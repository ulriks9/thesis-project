{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nyonk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from config import TrainingConfig\n",
    "from data import Data\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import HfFolder, Repository, whoami\n",
    "from huggingface_hub import notebook_login\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from unet import UNet\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_STEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((config.image_size, config.image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    images = [preprocess(image) for image in examples[\"image\"]]\n",
    "\n",
    "    return {\"images\": images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mnist (C:/Users/nyonk/.cache/huggingface/datasets/mnist/mnist/1.0.0/9d494b7f466d6931c64fb39d58bb1249a4d85c9eb9865d9bc20960b999e2a332)\n"
     ]
    }
   ],
   "source": [
    "data_loader = Data()\n",
    "dataset = data_loader.load_data(\"mnist\", length=1000)\n",
    "dataset.set_transform(transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(\n",
    "        config.image_size,\n",
    "        config.image_size,\n",
    "        2 * config.image_size,\n",
    "        2 * config.image_size, \n",
    "        4 * config.image_size,\n",
    "        4 * config.image_size\n",
    "    ),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\", \n",
    "        \"UpBlock2D\", \n",
    "        \"UpBlock2D\", \n",
    "        \"UpBlock2D\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(timesteps=TRAINING_STEPS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise Schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=TRAINING_STEPS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new('L', size=(cols * w, rows * h))\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "\n",
    "    return grid\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size = config.eval_batch_size, \n",
    "        generator=torch.manual_seed(config.seed),\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_repo_name(model_id: str, organization: str = None, token: str = None):\n",
    "    if token is None:\n",
    "        token = HfFolder.get_token()\n",
    "    if organization is None:\n",
    "        username = whoami(token)[\"name\"]\n",
    "        return f\"{username}/{model_id}\"\n",
    "    else:\n",
    "        return f\"{organization}/{model_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    "        log_with=\"tensorboard\",\n",
    "        logging_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        if config.push_to_hub:\n",
    "            repo_name = get_full_repo_name(Path(config.output_dir).name)\n",
    "            repo = Repository(config.output_dir, clone_from=repo_name)\n",
    "        elif config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "    \n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the \n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    global_step = 0\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch[\"images\"]\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                if config.push_to_hub:\n",
    "                    repo.push_to_hub(commit_message=f\"Epoch {epoch}\", blocking=True)\n",
    "                else:\n",
    "                    pipeline.save_pretrained(config.output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [00:43<00:00,  2.44it/s, loss=1.03, lr=1.94e-5, step=99]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value after * must be an iterable, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nyonk\\Documents\\GitHub\\thesis-project\\DDIM\\diffusion.ipynb Cell 18\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_loop(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     noise_scheduler\u001b[39m=\u001b[39;49mnoise_scheduler,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     lr_scheduler\u001b[39m=\u001b[39;49mlr_scheduler\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\nyonk\\Documents\\GitHub\\thesis-project\\DDIM\\diffusion.ipynb Cell 18\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m pipeline \u001b[39m=\u001b[39m DDPMPipeline(unet\u001b[39m=\u001b[39maccelerator\u001b[39m.\u001b[39munwrap_model(model), scheduler\u001b[39m=\u001b[39mnoise_scheduler)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m config\u001b[39m.\u001b[39msave_image_epochs \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m epoch \u001b[39m==\u001b[39m config\u001b[39m.\u001b[39mnum_epochs \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     evaluate(config, epoch, pipeline)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m config\u001b[39m.\u001b[39msave_model_epochs \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m epoch \u001b[39m==\u001b[39m config\u001b[39m.\u001b[39mnum_epochs \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mpush_to_hub:\n",
      "\u001b[1;32mc:\\Users\\nyonk\\Documents\\GitHub\\thesis-project\\DDIM\\diffusion.ipynb Cell 18\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(config, epoch, pipeline):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Sample some images from random noise (this is the backward diffusion process).\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# The default pipeline output type is `List[PIL.Image]`\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     images \u001b[39m=\u001b[39m pipeline(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         batch_size \u001b[39m=\u001b[39;49m config\u001b[39m.\u001b[39;49meval_batch_size, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         generator\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mmanual_seed(config\u001b[39m.\u001b[39;49mseed),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     )\u001b[39m.\u001b[39mimages\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Make a grid out of the images\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nyonk/Documents/GitHub/thesis-project/DDIM/diffusion.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     image_grid \u001b[39m=\u001b[39m make_grid(images, rows\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, cols\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nyonk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nyonk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\diffusers\\pipelines\\ddpm\\pipeline_ddpm.py:100\u001b[0m, in \u001b[0;36mDDPMPipeline.__call__\u001b[1;34m(self, batch_size, generator, num_inference_steps, output_type, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m     image_shape \u001b[39m=\u001b[39m (batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet\u001b[39m.\u001b[39min_channels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet\u001b[39m.\u001b[39msample_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet\u001b[39m.\u001b[39msample_size)\n\u001b[0;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 100\u001b[0m     image_shape \u001b[39m=\u001b[39m (batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet\u001b[39m.\u001b[39min_channels, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet\u001b[39m.\u001b[39msample_size)\n\u001b[0;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    103\u001b[0m     \u001b[39m# randn does not work reproducibly on mps\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(image_shape, generator\u001b[39m=\u001b[39mgenerator)\n",
      "\u001b[1;31mTypeError\u001b[0m: Value after * must be an iterable, not NoneType"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    lr_scheduler=lr_scheduler\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
